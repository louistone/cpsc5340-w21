{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Overview\n",
    "\n",
    "\n",
    "If the IR cycle is something like this:\n",
    "1. Collect documents (i.e. web crawling or retrieving specific pages)\n",
    "1. Extract *structured* information from documents (i.e. convert the document external format to match your schema)\n",
    "1. Index the documents\n",
    "1. Query the index\n",
    "\n",
    "Most of the documents of interest have some structured elements and some unstructured elements. \n",
    "We will look at Seattle U computer science faculty home pages.  For example, Prof. Dingle's page:\n",
    "https://www.seattleu.edu/scieng/computer-science/faculty-and-staff/dingle-adair.html\n",
    "\n",
    "Things like the name, email, office number, and phone number are structured, and the list of research interests is (probably) unstructured narrative. \n",
    "\n",
    "Here we are going to concentrate on the first two.  We will work on\n",
    "1. Making a service call to get an HTML document\n",
    "1. Parsing the document to pull out certain fields \n",
    "1. Packaging and storing document data so it's ready for indexing\n",
    "\n",
    "The exercise:  for any/all faculty pages, extract this information\n",
    "\n",
    "1. Name\n",
    "1. Phone number\n",
    "1. Email address\n",
    "1. Research interests\n",
    "\n",
    "Many sites have a \"structured API\"  -- for example https://docs.microsoft.com/en-us/linkedin/ -- which takes a request (e.g. for a person or handle) and returns a data structure (e.g. containing the person's name, contacts, employment history).  \n",
    "But sometimes we have to extract structured information directly from a web page -- that is tricky and dangerous, because the HTML is structured for display purposes and not semantically -- the HTML can change abruptly and break all your extraction code, and there is no guarantee that the structure of every page of interest is the same.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Service Calls\n",
    "\n",
    "Getting the HTML source for a page.\n",
    "\n",
    "We will be making calls to an HTTP server, so we need to talk about requests and responses.  This will be useful to you both in the retrieval context, but also because you will be making requests to SOLR, which is itself a service.\n",
    "\n",
    "We will use Python requests library http://docs.python-requests.org/en/master/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "url = \"https://www.seattleu.edu/scieng/computer-science/faculty-and-staff/dingle-adair.html\"\n",
    "response = requests.get(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response.status_code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response.headers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response.text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### HTML String to Parsed HTML\n",
    "\n",
    "Beautiful Soup package\n",
    "* Documentation:  https://www.crummy.com/software/BeautifulSoup/bs4/doc/\n",
    "* Installation: pip install beautifulsoup4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup as soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "page = soup(response.text, \"html.parser\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(page)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(page.title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(page.head)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(page.body)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(page.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(page.prettify())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## This extracts the name!\n",
    "page.find('div', {'id': 'zoneA'}).find('h1', {'id': 'pageTitle'}).text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## This extracts the email!!\n",
    "page.find('div', {'id': 'zoneA'}).\\\n",
    "find('div', {'class': \"staffBioPageInfo\"}).\\\n",
    "find('p', {'class': 'Email'}).\\\n",
    "find('a').\\\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## This extracts the phone!!\n",
    "page.find('div', {'id': 'zoneA'}).\\\n",
    "find('div', {'class': \"staffBioPageInfo\"}).\\\n",
    "find('p', {'class': 'Phone'}).\\\n",
    "text.replace('Phone: ', '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## This extracts the bio information!!!\n",
    "page.find('div', {'class': \"ExtendedBiography\"}).text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Packaging it Up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "handles = ['dingle-adair', 'mckee-michael', 'hanks-steven', 'khadivi-pejman', 'leblanc-richard']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_faculty_info(handle):\n",
    "    url = f\"https://www.seattleu.edu/scieng/computer-science/faculty-and-staff/{handle}.html\"\n",
    "    response = requests.get(url)\n",
    "    page = soup(response.text, \"html.parser\")\n",
    "    name = page.find('div', {'id': 'zoneA'}).find('h1', {'id': 'pageTitle'}).text\n",
    "    email = page.find('div', {'id': 'zoneA'}).find('div', {'class': \"staffBioPageInfo\"}).find('p', {'class': 'Email'})\n",
    "    if email == None:\n",
    "        email = None\n",
    "    else:\n",
    "        email = email.find('a').text\n",
    "    phone = page.find('div', {'id': 'zoneA'}).find('div', {'class': \"staffBioPageInfo\"}).find('p', {'class': 'Phone'})\n",
    "    if phone == None:\n",
    "        phone = None\n",
    "    else:\n",
    "        phone = phone.text.replace('Phone: ', '')\n",
    "    bio = page.find('div', {'class': \"ExtendedBiography\"})\n",
    "    if bio == None:\n",
    "        bio = None\n",
    "    else:\n",
    "        bio = bio.text\n",
    "    return {'name': name, 'email': email, 'phone': phone, 'bio': bio}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for handle in handles:\n",
    "    print(extract_faculty_info(handle))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Serializing / Storing\n",
    "\n",
    "It is often useful/necessary to store these \"documents\" prior to indexing.  Usually this consists of storing the URL or handle, and have it point to the parsed document.   That way a crawler can skip the page if it wants\n",
    "\n",
    "Two implementations\n",
    "* Quick and easy and efficient:  python \"pickle\" serializer.\n",
    "* Stil quick and easy to use but leaves us readable text for indexing:  write JSON string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "dingle = extract_faculty_info('dingle-adair')\n",
    "pickle.dump(dingle, open(\"stored/dingle-adair.p\", \"wb\"))\n",
    "recovered = pickle.load( open( \"stored/dingle-adair.p\", \"rb\" ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(recovered)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Put some aside for next lecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for handle in handles:\n",
    "    data = extract_faculty_info(handle)\n",
    "    print(str(data) + \"\\n\")\n",
    "    pickle.dump(data, open( f\"stored/{handle}.p\", \"wb\" ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Also put out a plain text version so we can use non-python tools "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = extract_faculty_info('dingle-adair')\n",
    "str(data)\n",
    "f = open(\"json\\dingle-adair.json\", \"w\")\n",
    "f.write(str(data))\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "for handle in handles:\n",
    "    data = extract_faculty_info(handle)\n",
    "    with open(f\"json/{handle}.json\", \"w\") as f:\n",
    "        json.dump(data, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
